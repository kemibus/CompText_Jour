---
title: "Week_11_Kemi"
author: "Kemi"
date: "2024-11-05"
output: html_document
---

```{r}
install.packages("pdftools")
library(tidyverse)
library(pdftools)
```


```{r}
#Remove split_file folder in a cleanup
text <- pdf_text("moley_news.PDF")
#pdf_text reads the text from a PDF file.
writeLines(text, "~/GitHub/CompText_Jour/Week_11_Assignment.txt")
#writeLines writes this text to a text file
```


```{r}
# Step 1: Read the entire text file into R

file_path <- "~/GitHub/CompText_Jour/Week_11_Assignment.txt"
text_data <- readLines(file_path)

# Step 2: Combine lines into one single string
text_combined <- paste(text_data, collapse = "\n")

# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "End of Document")[[1]]

# Step 4: Write each section to a new file
output_dir <- "~/GitHub/CompText_Jour/Week_11_Assignment/Week_11_extracted/"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("moley_extracted_", i, ".txt"))
  writeLines(documents[[i]], output_file)
}

cat("Files created:", length(documents), "\n")
```

```{r}
moley_index <- read_lines("~/GitHub/CompText_Jour/Week_11_Assignment/Week_11_extracted/moley_extracted_1.txt")
# Extract lines 6 to 176
extracted_lines <- moley_index[6:176]


# Print the extracted lines to the console
cat(extracted_lines, sep = "\n")

extracted_lines <- extracted_lines |> 
  as.data.frame() 
```


```{r}
# Step 1: Trim spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
  mutate(
    # Trim leading and trailing spaces before detection
    trimmed_line = str_trim(extracted_lines),  

    # Detect titles (start with a number and a period)
    is_title = str_detect(trimmed_line, "^\\d+\\. "),  

    # Detect dates (e.g., "Aug 14, 2024")
    is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
  )

# Step 2: Shift dates to align with corresponding titles
aligned_data <- cleaned_data |>
  mutate(
    date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
  ) |>
  filter(is_title) |>
  select(trimmed_line, date)  # Keep only the relevant columns

# Step 3: Rename columns for clarity
final_data <- aligned_data |>
  rename(
    title = trimmed_line,
    date = date
  )

#Step 4: Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")


#Step 5: Format date, clean headline
final_data <- final_data |> 
  mutate(date = as.Date(date2,format = "%b %d, %Y")) |> 
  mutate(title =str_remove(title, "^\\d+\\. ")) |> 
  subset(select = -(date2)) |> 
  mutate(index = row_number()) |> 
  select(index, date, title, publication)

write_csv(final_data, "Week_11_extracted/final_data.csv")
```


```{r}

# List out text files that match pattern .txt, create DF
files <- list.files("~/GitHub/CompText_Jour/Week_11_Assignment/Week_11_extracted", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an index with the file name
 mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))

#the actual path: ~/GitHub/CompText_Jour/Week_11_Assignment

#Join the file list to the index

#load final data if you haven't already
#final_data <- read.csv("assets/final_data.csv")

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
#you need the actual hard-coded path on this line below to the text
  
  # mutate(filepath = paste0("~/GitHub/CompText_Jour/Week_11_Assignment/", filename))
  mutate(filepath = paste0("~/GitHub/CompText_Jour/Week_11_Assignment/Week_11_extracted/", filename))
head(final_index)
```


```{r}
# Define function to loop through each text file 

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2) 
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

#Execute function using lapply

lapply(row_values, create_article_text)

###
# Clean up articles_df and join to index dataframe
###

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

#After viewing articles_df, I see 158 lines from the index that I don't need. Cutting them 

articles_df <- articles_df %>%
  slice(-c(1:158)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

#write.csv(articles_df, "Week_11_extracted.csv")
```
```


```{r}
```{r}
#load tidyverse, tidytext, rio and quanteda libraries
library(tidyverse)
library(rio)
library(tidytext)
library(quanteda)
library(knitr)
library(dplyr)

```
#Remove stopwords
```{r}
data(stop_words)
```

```{r}
bigrams <- articles_df %>%
  select(sentence) %>%
  mutate(sentence = str_squish(sentence)) %>%
  mutate(sentence = tolower(sentence)) %>%
  mutate(sentence = str_replace_all(sentence, 
    "title|pages|publication date|publication type|web publication|issn|language of publication: english|publication type|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\|About LexisNexis|erms & Conditions|Copyright|2020 LexisNexis|Loose Ends|Power Line|All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note|https|wwwalt|Author\\.", 
    "")) %>%
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ", extra = "merge", fill = "right") %>%
  filter(!is.na(word1) & !is.na(word2)) %>%
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE)
```


```{r}
# This is a different method of removing the metadata that i tried but still couldn't get them off the dataset.
# Define patterns to remove unwanted metadata
metadata_patterns <- paste(
  "title|pages|publication date|publication type|web publication|issn",
  "language of publication: english|document url|copyright|last updated|database",
  "startofarticle|proquest document id|classification|https|--|people|publication info",
  "illustration|caption|[0-9]+|identifier /keyword|erms & Conditions|Copyright",
  "All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note",
  "Author\\.|https|wwwalt",
  sep = "|"
)

# Process bigrams
bigrams <- articles_df %>%
  select(sentence) %>%
  mutate(sentence = str_squish(sentence)) %>%
  mutate(sentence = tolower(sentence)) %>%
  mutate(sentence = str_remove_all(sentence, metadata_patterns)) %>%
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ", extra = "merge", fill = "right") %>%
  filter(!is.na(word1) & !is.na(word2)) %>%
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE)
```


```{r}
# top_20_bigrams
top_20_bigrams <- bigrams %>% 
   slice_max(n, n = 20) %>% 
   mutate(bigram = paste(word1, " ", word2)) %>% 
   select(bigram, n)

top_20_bigrams

```

```{r}
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Moley Articles",
       caption = "n=33 articles. Graphic by Kemi Busari 13-05-2024",
       x = "Bigrams",
       y = "Count of terms")
```



```{r}
#memo
•	The dataframe, after applying the articles_df function, emerged in 4563 columns and 14 rows. The columns included sentences from each dataframe while the rows have identifier information such as filename, path and other information to trace and identify the sentences.
•	After indexing, a peep into the dataset shows that lines 1 to 158 are blank rows and full of unwanted metadata. This was cleaned out before the articles were written into csv files.
•	The write.csv function returned 32 distinct articles disaggregated to 6529 rows.
•	The unnest token, filter and count functions combined to give me 7424 bigrams.
•	The top 3 bigrams are: “gold standard” with 46 entries “white house” with 46 and “Raymond Moley” with 40. This suggests a prominence of conversation around politics, economy and the main Raymond Moley, being the main actor in the articles.
•	The major challenge came in trying to strip the dataset of unwanted metadata.  I took the time to identify all phrases that need to be removed from the dataset but the “mutate” function doesn’t seem to work for me, even after trying with another method. This impacted my findings in the next steps but I’m confident of my understanding the process, regardless. Going through this process again gives me a better confidence of being able to handle the cleaning of my own data. The process of creating a filepath not wired to my computer now comes more easily as well as the process of creating folders for specific projects and assigning the folder as working directory.


```